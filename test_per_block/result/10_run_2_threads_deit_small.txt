

Measuring average inference latecy of PatchEmbed over 10 run.
  - compile Model = False
  - batch size = 1
  - transformer blocks = PatchEmbed
  - nums_threads = 2
Inference time for 12 blocks: iteration_0 = 0.043 || average = 0.043 seconds 
Inference time for 12 blocks: iteration_1 = 0.031 || average = 0.037 seconds 
Inference time for 12 blocks: iteration_2 = 0.026 || average = 0.033 seconds 
Inference time for 12 blocks: iteration_3 = 0.031 || average = 0.033 seconds 
Inference time for 12 blocks: iteration_4 = 0.025 || average = 0.031 seconds 
Inference time for 12 blocks: iteration_5 = 0.042 || average = 0.033 seconds 
Inference time for 12 blocks: iteration_6 = 0.048 || average = 0.035 seconds 
Inference time for 12 blocks: iteration_7 = 0.043 || average = 0.036 seconds 
Inference time for 12 blocks: iteration_8 = 0.045 || average = 0.037 seconds 
Inference time for 12 blocks: iteration_9 = 0.026 || average = 0.036 seconds 


Measuring average inference latecy of deit_small_patch16_224 over 10 run.
  - compile Model = False
  - batch size = 1
  - transformer blocks = 12
  - nums_threads = 2
Inference time for 12 blocks: iteration_0 = 1.980 || average = 1.980 seconds 
Inference time for 12 blocks: iteration_1 = 1.528 || average = 1.754 seconds 
Inference time for 12 blocks: iteration_2 = 1.789 || average = 1.766 seconds 
Inference time for 12 blocks: iteration_3 = 1.184 || average = 1.620 seconds 
Inference time for 12 blocks: iteration_4 = 1.868 || average = 1.670 seconds 
Inference time for 12 blocks: iteration_5 = 1.260 || average = 1.601 seconds 
Inference time for 12 blocks: iteration_6 = 1.798 || average = 1.630 seconds 
Inference time for 12 blocks: iteration_7 = 1.297 || average = 1.588 seconds 
Inference time for 12 blocks: iteration_8 = 1.845 || average = 1.617 seconds 
Inference time for 12 blocks: iteration_9 = 1.241 || average = 1.579 seconds 


Measuring average inference latecy of deit_small_patch16_224 over 10 run.
  - compile Model = False
  - batch size = 1
  - transformer blocks = 1
  - nums_threads = 2
Inference time for 1 blocks: iteration_0 = 0.174 || average = 0.174 seconds 
Inference time for 1 blocks: iteration_1 = 0.176 || average = 0.175 seconds 
Inference time for 1 blocks: iteration_2 = 0.189 || average = 0.179 seconds 
Inference time for 1 blocks: iteration_3 = 0.175 || average = 0.178 seconds 
Inference time for 1 blocks: iteration_4 = 0.173 || average = 0.177 seconds 
Inference time for 1 blocks: iteration_5 = 0.174 || average = 0.177 seconds 
Inference time for 1 blocks: iteration_6 = 0.182 || average = 0.178 seconds 
Inference time for 1 blocks: iteration_7 = 0.183 || average = 0.178 seconds 
Inference time for 1 blocks: iteration_8 = 0.178 || average = 0.178 seconds 
Inference time for 1 blocks: iteration_9 = 0.174 || average = 0.178 seconds 
--- Latency(sec) ---
transformer block + patchEmbed: 1 block = 0.17778651899425313 || 12 block = 1.578970585297793
 - speedup (12 block + 1 patchEmbed)/(1 block + 1 patchEmbed) = 8.88

transformer block: 1 block = 0.14176955010043457 || 12 block = 1.5429536164039745
 - speedup (12 block/ 1 block) = 10.88
