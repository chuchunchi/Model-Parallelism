{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For synchronous execution of CPU and GPU\n",
    "device = torch.device(\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-13 15:54:18,968] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from modeling_mixformer_sequential import MixFormerSequentialForCausalLM, InferenceParams\n",
    "from configuration_mixformer_sequential import MixFormerSequentialConfig\n",
    "\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torch\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "class ClientSideMixFormerSequentialForCausalLM(MixFormerSequentialForCausalLM):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.split_layer=2\n",
    "        self.num_layers=20\n",
    "        for i in range(self.split_layer, self.num_layers+1):\n",
    "            self.layers[i] = None\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        past_key_values: Optional[Union[torch.FloatTensor, InferenceParams]] = None,\n",
    "        attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if attention_mask is not None and self.training:\n",
    "            print(\"`attention_mask` is not supported during training. Using it might lead to unexpected results.\")\n",
    "\n",
    "        if past_key_values is None and attention_mask is None:\n",
    "            print(\"[Client] past_key_values & attention_mask is None!\")\n",
    "            lm_logits = self.layers(input_ids)\n",
    "            return lm_logits\n",
    "        else:\n",
    "            print(\"[Client] forward with past_key_values or attention_mask!\")\n",
    "            hidden_layer = self.layers[0](input_ids)\n",
    "            for module in self.layers[1: self.split_layer]:  # return intermediate tensor\n",
    "                hidden_layer = module(hidden_layer, past_key_values=past_key_values, attention_mask=attention_mask)\n",
    "            return input_ids, past_key_values, attention_mask, labels, hidden_layer\n",
    "\n",
    "\n",
    "\n",
    "class ServerSideMixFormerSequentialForCausalLM(MixFormerSequentialForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.split_layer=2\n",
    "        for i in range(0, self.split_layer):\n",
    "            self.layers[i] = None\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        past_key_values: Optional[Union[torch.FloatTensor, InferenceParams]] = None,\n",
    "        attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        hidden_layer_input: torch.Tensor = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        if attention_mask is not None and self.training:\n",
    "            print(\"`attention_mask` is not supported during training. Using it might lead to unexpected results.\")\n",
    "\n",
    "        if past_key_values is None and attention_mask is None:\n",
    "            print(\"[Server] past_key_values & attention_mask is None!\")\n",
    "            lm_logits = self.layers(input_ids)\n",
    "        else:\n",
    "            print(\"[Server] forward with past_key_values or attention_mask!\")\n",
    "            hidden_layer = hidden_layer_input\n",
    "            for module in self.layers[self.split_layer:-1]:  # Compute the remaining block \n",
    "                hidden_layer = module(hidden_layer, past_key_values=past_key_values, attention_mask=attention_mask)\n",
    "            lm_logits = self.layers[-1](hidden_layer)\n",
    "            \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss(lm_logits, labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(loss=loss, logits=lm_logits, past_key_values=past_key_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MixFormerSequentialConfig()\n",
    "client_model = ClientSideMixFormerSequentialForCausalLM(config)\n",
    "server_model = ServerSideMixFormerSequentialForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(51200, 2048)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (2): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (3): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (4): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (5): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (6): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (7): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (8): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (9): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (10): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (11): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (12): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (13): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (14): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (15): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (16): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (17): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (18): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (19): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (20): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (21): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (22): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (23): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (24): ParallelBlock(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (inner_attn): SelfAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (inner_cross_attn): CrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "    )\n",
      "    (25): CausalLMHead(\n",
      "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "model_name = (\"gpt2\", \"microsoft/phi-1_5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name[1], \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[1],\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(model)\n",
    "lora_modules=[\"Wqkv\"] \n",
    "lora_config = LoraConfig(\n",
    "    r=2,  # dimension of the updated matrices\n",
    "    lora_alpha=64,  # parameter for scaling\n",
    "    target_modules=lora_modules,\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "client_model = get_peft_model(client_model, lora_config)\n",
    "server_model = get_peft_model(server_model, lora_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
